{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import re\n",
    "import emoji"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv('Data/train.csv')\n",
    "test_df = pd.read_csv('Data/test.csv')\n",
    "sampledf = pd.read_csv('Data/sample_submission.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    text = re.sub(r'[^\\w\\s]', '', text)  # Remove special characters\n",
    "    text = re.sub(r',', '', text)  # Remove commas\n",
    "    text = emoji.replace_emoji(text, replace='')  # Remove emojis\n",
    "    return text\n",
    "\n",
    "# Apply the clean_text function to the 'text' column\n",
    "train_df['text'] = train_df['text'].apply(clean_text)\n",
    "test_df['text'] = test_df['text'].apply(clean_text)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>keyword</th>\n",
       "      <th>location</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Just happened a terrible car crash</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Heard about earthquake is different cities sta...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>there is a forest fire at spot pond geese are ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>9</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Apocalypse lighting Spokane wildfires</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>11</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Typhoon Soudelor kills 28 in China and Taiwan</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id keyword location                                               text\n",
       "0   0     NaN      NaN                 Just happened a terrible car crash\n",
       "1   2     NaN      NaN  Heard about earthquake is different cities sta...\n",
       "2   3     NaN      NaN  there is a forest fire at spot pond geese are ...\n",
       "3   9     NaN      NaN              Apocalypse lighting Spokane wildfires\n",
       "4  11     NaN      NaN      Typhoon Soudelor kills 28 in China and Taiwan"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7613, 5)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2218057549a44466bc855e8f4b169912",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/48.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ae010e72d1094a3290607aa794cbb5ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/571 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d77377332c8f49cc861fcfdeaf1a3a98",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/226k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ce9e9c8b06474cc08f35304c15027b10",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/455k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "93172bd7f5434aa684d9837047838080",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading:   0%|          | 0.00/1.37G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:1295: UserWarning: Layer 'tf_bert_model' looks like it has unbuilt state, but Keras is not able to trace the layer `call()` in order to build it automatically. Possible causes:\n",
      "1. The `call()` method of your layer may be crashing. Try to `__call__()` the layer eagerly on some test input first to see if it works. E.g. `x = np.random.random((3, 4)); y = layer(x)`\n",
      "2. If the `call()` method is correct, then you may need to implement the `def build(self, input_shape)` method on your layer. It should create all variables used by the layer (e.g. by calling `layer.build()` on all its children layers).\n",
      "Exception encountered: ''The following keyword arguments are not supported by this model: ['kwargs'].''\n",
      "  warnings.warn(\n",
      "c:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\layers\\layer.py:361: UserWarning: `build()` was called on layer 'tf_bert_model', however the layer does not have a `build()` method implemented and it looks like it has unbuilt state. This will cause the layer to be marked as built, despite not being actually built, which may cause failures down the line. Make sure to implement a proper `build()` method.\n",
      "  warnings.warn(\n",
      "Some layers from the model checkpoint at bert-large-uncased were not used when initializing TFBertModel: ['nsp___cls', 'mlm___cls', 'bert/encoder/layer_._11/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._21/intermediate/dense/bias:0', 'bert/pooler/dense/kernel:0', 'bert/encoder/layer_._15/attention/self/value/bias:0', 'bert/encoder/layer_._10/intermediate/dense/kernel:0', 'bert/encoder/layer_._13/output/dense/kernel:0', 'bert/encoder/layer_._0/output/dense/bias:0', 'bert/encoder/layer_._14/attention/self/key/kernel:0', 'bert/encoder/layer_._16/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._16/attention/self/query/kernel:0', 'bert/encoder/layer_._20/output/dense/kernel:0', 'bert/encoder/layer_._14/intermediate/dense/bias:0', 'bert/encoder/layer_._14/output/dense/kernel:0', 'bert/encoder/layer_._16/output/dense/bias:0', 'bert/encoder/layer_._22/attention/self/key/bias:0', 'bert/encoder/layer_._8/attention/self/key/kernel:0', 'bert/encoder/layer_._14/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._18/attention/self/value/bias:0', 'bert/encoder/layer_._21/attention/output/dense/kernel:0', 'bert/encoder/layer_._16/output/LayerNorm/gamma:0', 'bert/encoder/layer_._23/attention/self/key/bias:0', 'bert/encoder/layer_._15/attention/self/key/bias:0', 'bert/encoder/layer_._18/attention/self/query/kernel:0', 'bert/encoder/layer_._17/attention/output/dense/bias:0', 'bert/encoder/layer_._20/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._21/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/output/dense/kernel:0', 'bert/encoder/layer_._14/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._13/attention/self/key/bias:0', 'bert/encoder/layer_._23/output/dense/bias:0', 'bert/encoder/layer_._7/attention/self/value/kernel:0', 'bert/encoder/layer_._2/attention/self/value/kernel:0', 'bert/encoder/layer_._4/attention/self/query/kernel:0', 'bert/encoder/layer_._9/output/dense/bias:0', 'bert/encoder/layer_._10/output/LayerNorm/gamma:0', 'bert/encoder/layer_._4/attention/self/value/kernel:0', 'bert/encoder/layer_._19/output/dense/bias:0', 'bert/encoder/layer_._16/attention/output/dense/bias:0', 'bert/encoder/layer_._21/attention/self/query/kernel:0', 'bert/encoder/layer_._0/attention/self/query/bias:0', 'bert/encoder/layer_._20/attention/self/value/kernel:0', 'bert/embeddings/LayerNorm/beta:0', 'bert/encoder/layer_._12/attention/self/query/bias:0', 'bert/encoder/layer_._17/attention/output/dense/kernel:0', 'bert/encoder/layer_._22/intermediate/dense/bias:0', 'bert/encoder/layer_._13/intermediate/dense/bias:0', 'bert/encoder/layer_._0/attention/self/query/kernel:0', 'bert/encoder/layer_._9/intermediate/dense/bias:0', 'bert/encoder/layer_._7/attention/self/query/kernel:0', 'bert/encoder/layer_._8/attention/self/query/bias:0', 'bert/encoder/layer_._13/attention/output/dense/kernel:0', 'bert/encoder/layer_._23/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/attention/output/dense/kernel:0', 'bert/encoder/layer_._16/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._3/attention/self/query/bias:0', 'bert/encoder/layer_._11/intermediate/dense/bias:0', 'bert/encoder/layer_._6/attention/output/dense/kernel:0', 'bert/encoder/layer_._22/output/LayerNorm/gamma:0', 'bert/encoder/layer_._12/attention/output/dense/kernel:0', 'bert/encoder/layer_._8/output/LayerNorm/gamma:0', 'bert/encoder/layer_._19/attention/output/dense/bias:0', 'bert/encoder/layer_._10/attention/self/query/kernel:0', 'bert/encoder/layer_._11/output/dense/bias:0', 'bert/encoder/layer_._17/attention/self/value/kernel:0', 'bert/encoder/layer_._13/attention/self/key/kernel:0', 'bert/encoder/layer_._1/attention/output/dense/bias:0', 'bert/encoder/layer_._7/intermediate/dense/bias:0', 'bert/encoder/layer_._20/attention/self/query/bias:0', 'bert/encoder/layer_._4/attention/output/dense/bias:0', 'bert/encoder/layer_._13/output/LayerNorm/gamma:0', 'bert/embeddings/word_embeddings/weight:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._9/attention/output/dense/bias:0', 'bert/encoder/layer_._9/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/self/key/kernel:0', 'bert/encoder/layer_._22/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._8/attention/self/key/bias:0', 'bert/encoder/layer_._16/attention/self/query/bias:0', 'bert/encoder/layer_._17/output/dense/kernel:0', 'bert/encoder/layer_._19/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._15/attention/output/dense/kernel:0', 'bert/encoder/layer_._5/output/LayerNorm/gamma:0', 'bert/encoder/layer_._22/intermediate/dense/kernel:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/intermediate/dense/kernel:0', 'bert/encoder/layer_._19/intermediate/dense/bias:0', 'bert/encoder/layer_._11/attention/self/query/kernel:0', 'bert/encoder/layer_._23/attention/self/key/kernel:0', 'bert/encoder/layer_._23/intermediate/dense/bias:0', 'bert/encoder/layer_._20/output/dense/bias:0', 'bert/encoder/layer_._2/attention/self/query/kernel:0', 'bert/encoder/layer_._20/attention/self/key/kernel:0', 'bert/encoder/layer_._18/attention/output/dense/bias:0', 'bert/encoder/layer_._11/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/attention/self/query/bias:0', 'bert/encoder/layer_._1/attention/self/value/kernel:0', 'bert/encoder/layer_._23/attention/output/dense/bias:0', 'bert/encoder/layer_._15/attention/output/dense/bias:0', 'bert/encoder/layer_._2/output/LayerNorm/beta:0', 'bert/encoder/layer_._19/attention/self/query/kernel:0', 'bert/encoder/layer_._6/output/dense/kernel:0', 'bert/encoder/layer_._12/intermediate/dense/kernel:0', 'bert/encoder/layer_._16/attention/self/key/bias:0', 'bert/encoder/layer_._2/output/dense/kernel:0', 'bert/encoder/layer_._1/intermediate/dense/kernel:0', 'bert/encoder/layer_._8/intermediate/dense/kernel:0', 'bert/encoder/layer_._6/output/dense/bias:0', 'bert/encoder/layer_._10/output/dense/kernel:0', 'bert/encoder/layer_._16/output/LayerNorm/beta:0', 'bert/encoder/layer_._12/output/LayerNorm/gamma:0', 'bert/encoder/layer_._23/attention/self/query/kernel:0', 'bert/encoder/layer_._11/output/LayerNorm/gamma:0', 'bert/encoder/layer_._19/output/LayerNorm/gamma:0', 'bert/encoder/layer_._7/output/dense/kernel:0', 'bert/encoder/layer_._19/attention/self/key/kernel:0', 'bert/encoder/layer_._11/attention/self/value/bias:0', 'bert/encoder/layer_._7/attention/self/key/kernel:0', 'bert/encoder/layer_._2/output/dense/bias:0', 'bert/encoder/layer_._15/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/attention/self/key/bias:0', 'bert/encoder/layer_._21/attention/self/key/kernel:0', 'bert/encoder/layer_._3/attention/self/key/kernel:0', 'bert/encoder/layer_._19/attention/self/value/kernel:0', 'bert/encoder/layer_._6/attention/self/query/kernel:0', 'bert/encoder/layer_._13/attention/self/value/bias:0', 'bert/encoder/layer_._21/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/intermediate/dense/bias:0', 'bert/encoder/layer_._21/output/dense/bias:0', 'bert/encoder/layer_._16/intermediate/dense/kernel:0', 'bert/encoder/layer_._4/output/LayerNorm/beta:0', 'bert/encoder/layer_._20/attention/self/value/bias:0', 'bert/encoder/layer_._5/attention/self/key/kernel:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._14/attention/output/dense/bias:0', 'bert/encoder/layer_._11/attention/self/query/bias:0', 'bert/encoder/layer_._17/attention/self/key/bias:0', 'bert/encoder/layer_._22/attention/output/dense/bias:0', 'bert/encoder/layer_._8/attention/self/value/kernel:0', 'bert/encoder/layer_._1/attention/self/value/bias:0', 'bert/encoder/layer_._6/attention/self/value/kernel:0', 'bert/encoder/layer_._3/attention/self/key/bias:0', 'bert/encoder/layer_._15/intermediate/dense/kernel:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._13/attention/self/query/bias:0', 'bert/encoder/layer_._17/output/LayerNorm/beta:0', 'bert/encoder/layer_._23/intermediate/dense/kernel:0', 'bert/encoder/layer_._6/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/attention/self/value/bias:0', 'bert/encoder/layer_._8/attention/self/query/kernel:0', 'bert/encoder/layer_._22/output/dense/bias:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._2/output/LayerNorm/gamma:0', 'bert/encoder/layer_._21/attention/self/value/kernel:0', 'bert/encoder/layer_._22/attention/self/key/kernel:0', 'bert/encoder/layer_._2/intermediate/dense/bias:0', 'bert/encoder/layer_._2/attention/self/query/bias:0', 'bert/encoder/layer_._15/attention/self/key/kernel:0', 'bert/encoder/layer_._0/attention/self/value/kernel:0', 'bert/encoder/layer_._7/attention/output/dense/bias:0', 'bert/encoder/layer_._3/attention/output/dense/kernel:0', 'bert/encoder/layer_._15/attention/self/query/bias:0', 'bert/encoder/layer_._21/attention/self/key/bias:0', 'bert/encoder/layer_._17/output/LayerNorm/gamma:0', 'bert/encoder/layer_._18/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._15/attention/self/query/kernel:0', 'bert/encoder/layer_._13/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._13/intermediate/dense/kernel:0', 'bert/encoder/layer_._20/output/LayerNorm/gamma:0', 'bert/encoder/layer_._17/output/dense/bias:0', 'bert/encoder/layer_._14/attention/self/query/bias:0', 'bert/encoder/layer_._22/attention/self/query/kernel:0', 'bert/encoder/layer_._1/attention/self/query/bias:0', 'bert/encoder/layer_._17/intermediate/dense/bias:0', 'bert/encoder/layer_._18/attention/self/key/bias:0', 'bert/encoder/layer_._3/intermediate/dense/bias:0', 'bert/encoder/layer_._7/attention/output/dense/kernel:0', 'bert/encoder/layer_._1/output/LayerNorm/beta:0', 'bert/encoder/layer_._14/attention/self/query/kernel:0', 'bert/encoder/layer_._23/output/dense/kernel:0', 'bert/encoder/layer_._1/intermediate/dense/bias:0', 'bert/encoder/layer_._9/attention/self/query/kernel:0', 'bert/encoder/layer_._13/attention/self/value/kernel:0', 'bert/encoder/layer_._21/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._10/attention/self/key/bias:0', 'bert/encoder/layer_._20/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/output/LayerNorm/gamma:0', 'bert/encoder/layer_._21/attention/self/value/bias:0', 'bert/encoder/layer_._21/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/output/LayerNorm/gamma:0', 'bert/encoder/layer_._12/attention/self/query/kernel:0', 'bert/encoder/layer_._19/attention/output/dense/kernel:0', 'bert/encoder/layer_._10/attention/self/query/bias:0', 'bert/encoder/layer_._10/attention/self/value/kernel:0', 'bert/encoder/layer_._16/output/dense/kernel:0', 'bert/encoder/layer_._4/intermediate/dense/bias:0', 'bert/embeddings/position_embeddings/embeddings:0', 'bert/encoder/layer_._4/attention/self/value/bias:0', 'bert/encoder/layer_._6/attention/self/value/bias:0', 'bert/encoder/layer_._17/attention/self/query/bias:0', 'bert/encoder/layer_._10/attention/self/value/bias:0', 'bert/encoder/layer_._18/attention/self/query/bias:0', 'bert/encoder/layer_._12/attention/self/key/bias:0', 'bert/encoder/layer_._3/intermediate/dense/kernel:0', 'bert/encoder/layer_._11/output/dense/kernel:0', 'bert/encoder/layer_._12/output/dense/kernel:0', 'bert/encoder/layer_._8/intermediate/dense/bias:0', 'bert/encoder/layer_._23/attention/self/value/bias:0', 'bert/encoder/layer_._14/attention/self/value/kernel:0', 'bert/encoder/layer_._7/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._14/attention/self/key/bias:0', 'bert/encoder/layer_._10/output/dense/bias:0', 'bert/encoder/layer_._11/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/output/dense/bias:0', 'bert/encoder/layer_._14/intermediate/dense/kernel:0', 'bert/encoder/layer_._20/attention/output/dense/bias:0', 'bert/encoder/layer_._3/output/dense/kernel:0', 'bert/encoder/layer_._15/output/dense/kernel:0', 'bert/encoder/layer_._22/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._17/attention/output/LayerNorm/gamma:0', 'bert/embeddings/LayerNorm/gamma:0', 'bert/encoder/layer_._16/attention/self/key/kernel:0', 'bert/encoder/layer_._12/output/LayerNorm/beta:0', 'bert/embeddings/token_type_embeddings/embeddings:0', 'bert/encoder/layer_._17/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/value/bias:0', 'bert/encoder/layer_._20/attention/self/query/kernel:0', 'bert/encoder/layer_._19/attention/self/value/bias:0', 'bert/encoder/layer_._8/output/dense/kernel:0', 'bert/encoder/layer_._10/intermediate/dense/bias:0', 'bert/encoder/layer_._21/attention/output/dense/bias:0', 'bert/encoder/layer_._0/attention/output/dense/bias:0', 'bert/encoder/layer_._0/attention/self/key/kernel:0', 'bert/encoder/layer_._9/output/LayerNorm/gamma:0', 'bert/encoder/layer_._17/attention/self/value/bias:0', 'bert/encoder/layer_._19/attention/self/query/bias:0', 'bert/encoder/layer_._15/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/attention/output/dense/kernel:0', 'bert/encoder/layer_._9/attention/self/query/bias:0', 'bert/encoder/layer_._7/output/LayerNorm/gamma:0', 'bert/encoder/layer_._4/output/dense/kernel:0', 'bert/encoder/layer_._7/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/output/dense/kernel:0', 'bert/encoder/layer_._20/intermediate/dense/bias:0', 'bert/encoder/layer_._3/attention/self/value/kernel:0', 'bert/encoder/layer_._14/output/LayerNorm/beta:0', 'bert/encoder/layer_._20/output/LayerNorm/beta:0', 'bert/encoder/layer_._14/attention/self/value/bias:0', 'bert/encoder/layer_._0/intermediate/dense/kernel:0', 'bert/encoder/layer_._17/attention/self/query/kernel:0', 'bert/encoder/layer_._11/attention/self/key/bias:0', 'bert/encoder/layer_._23/attention/output/dense/kernel:0', 'bert/encoder/layer_._18/output/dense/bias:0', 'bert/encoder/layer_._19/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._0/attention/self/value/bias:0', 'bert/encoder/layer_._2/attention/self/value/bias:0', 'bert/encoder/layer_._22/attention/self/query/bias:0', 'bert/encoder/layer_._22/attention/self/value/bias:0', 'bert/encoder/layer_._13/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._15/attention/self/value/kernel:0', 'bert/encoder/layer_._5/attention/self/key/bias:0', 'bert/encoder/layer_._14/output/LayerNorm/gamma:0', 'bert/encoder/layer_._2/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/attention/output/dense/bias:0', 'bert/encoder/layer_._21/output/LayerNorm/beta:0', 'bert/encoder/layer_._18/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/self/key/bias:0', 'bert/encoder/layer_._12/output/dense/bias:0', 'bert/encoder/layer_._15/output/LayerNorm/beta:0', 'bert/encoder/layer_._11/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/intermediate/dense/kernel:0', 'bert/encoder/layer_._3/attention/self/query/kernel:0', 'bert/encoder/layer_._5/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/self/key/kernel:0', 'bert/encoder/layer_._12/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._18/attention/self/value/kernel:0', 'bert/encoder/layer_._19/output/dense/kernel:0', 'bert/encoder/layer_._0/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._16/attention/output/dense/kernel:0', 'bert/encoder/layer_._10/output/LayerNorm/beta:0', 'bert/encoder/layer_._4/attention/self/key/kernel:0', 'bert/encoder/layer_._11/attention/self/key/kernel:0', 'bert/encoder/layer_._13/attention/self/query/kernel:0', 'bert/encoder/layer_._23/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/attention/self/value/bias:0', 'bert/encoder/layer_._2/attention/output/dense/bias:0', 'bert/encoder/layer_._22/attention/self/value/kernel:0', 'bert/encoder/layer_._17/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/output/dense/bias:0', 'bert/encoder/layer_._17/attention/self/key/kernel:0', 'bert/encoder/layer_._1/attention/self/key/bias:0', 'bert/encoder/layer_._1/attention/self/query/kernel:0', 'bert/encoder/layer_._23/attention/self/query/bias:0', 'bert/encoder/layer_._8/attention/output/dense/bias:0', 'bert/encoder/layer_._20/attention/self/key/bias:0', 'bert/encoder/layer_._1/output/dense/kernel:0', 'bert/encoder/layer_._18/attention/output/dense/kernel:0', 'bert/encoder/layer_._4/intermediate/dense/kernel:0', 'bert/encoder/layer_._8/attention/output/dense/kernel:0', 'bert/encoder/layer_._18/intermediate/dense/bias:0', 'bert/encoder/layer_._14/attention/output/dense/kernel:0', 'bert/encoder/layer_._7/output/dense/bias:0', 'bert/encoder/layer_._12/attention/self/value/bias:0', 'bert/encoder/layer_._9/attention/self/value/bias:0', 'bert/encoder/layer_._21/attention/self/query/bias:0', 'bert/encoder/layer_._6/attention/self/key/bias:0', 'bert/encoder/layer_._6/attention/output/dense/bias:0', 'bert/encoder/layer_._13/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._16/attention/self/value/kernel:0', 'bert/encoder/layer_._3/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._15/intermediate/dense/bias:0', 'bert/encoder/layer_._19/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/self/query/kernel:0', 'bert/encoder/layer_._6/output/LayerNorm/gamma:0', 'bert/encoder/layer_._7/attention/self/query/bias:0', 'bert/encoder/layer_._6/intermediate/dense/bias:0', 'bert/encoder/layer_._6/attention/self/key/kernel:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/output/LayerNorm/beta:0', 'bert/encoder/layer_._18/output/LayerNorm/gamma:0', 'bert/encoder/layer_._0/output/dense/kernel:0', 'bert/encoder/layer_._1/attention/self/key/kernel:0', 'bert/encoder/layer_._14/output/dense/bias:0', 'bert/encoder/layer_._22/output/LayerNorm/beta:0', 'bert/encoder/layer_._3/output/LayerNorm/beta:0', 'bert/encoder/layer_._12/intermediate/dense/bias:0', 'bert/encoder/layer_._1/output/LayerNorm/gamma:0', 'bert/encoder/layer_._22/output/dense/kernel:0', 'bert/encoder/layer_._2/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._23/attention/self/value/kernel:0', 'bert/encoder/layer_._12/attention/self/value/kernel:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._15/output/dense/bias:0', 'bert/encoder/layer_._20/attention/output/dense/kernel:0', 'bert/encoder/layer_._0/attention/self/key/bias:0', 'bert/encoder/layer_._20/intermediate/dense/kernel:0', 'bert/encoder/layer_._22/attention/output/dense/kernel:0', 'bert/encoder/layer_._23/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/intermediate/dense/kernel:0', 'bert/encoder/layer_._5/attention/output/dense/bias:0', 'bert/encoder/layer_._9/attention/self/value/kernel:0', 'bert/encoder/layer_._11/intermediate/dense/kernel:0', 'bert/encoder/layer_._8/output/LayerNorm/beta:0', 'bert/encoder/layer_._5/attention/self/query/bias:0', 'bert/encoder/layer_._18/attention/self/key/kernel:0', 'bert/encoder/layer_._1/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._1/attention/output/dense/kernel:0', 'bert/encoder/layer_._8/attention/self/value/bias:0', 'bert/encoder/layer_._12/attention/self/key/kernel:0', 'bert/encoder/layer_._10/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._10/attention/self/key/kernel:0', 'bert/encoder/layer_._5/attention/self/value/kernel:0', 'bert/encoder/layer_._19/intermediate/dense/kernel:0', 'bert/encoder/layer_._16/intermediate/dense/bias:0', 'bert/encoder/layer_._4/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._6/attention/self/query/bias:0', 'bert/pooler/dense/bias:0', 'bert/encoder/layer_._18/intermediate/dense/kernel:0', 'bert/encoder/layer_._18/output/LayerNorm/beta:0', 'bert/encoder/layer_._16/attention/self/value/bias:0', 'bert/encoder/layer_._6/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._19/attention/self/key/bias:0', 'bert/encoder/layer_._18/output/dense/kernel:0', 'bert/encoder/layer_._10/attention/output/dense/bias:0', 'bert/encoder/layer_._15/output/LayerNorm/gamma:0', 'bert/encoder/layer_._12/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._7/output/LayerNorm/beta:0', 'bert/encoder/layer_._9/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._11/attention/self/value/kernel:0', 'bert/encoder/layer_._11/attention/output/dense/bias:0', 'bert/encoder/layer_._7/attention/self/key/bias:0', 'bert/encoder/layer_._4/output/dense/bias:0', 'bert/encoder/layer_._13/attention/output/dense/bias:0', 'bert/encoder/layer_._8/output/dense/bias:0', 'bert/encoder/layer_._4/attention/self/key/bias:0', 'bert/encoder/layer_._1/output/dense/bias:0', 'bert/encoder/layer_._23/attention/output/LayerNorm/gamma:0', 'bert/encoder/layer_._21/output/dense/kernel:0', 'bert/encoder/layer_._13/output/dense/bias:0', 'bert/encoder/layer_._5/intermediate/dense/bias:0', 'bert/encoder/layer_._4/attention/output/dense/kernel:0', 'bert/encoder/layer_._8/attention/output/LayerNorm/beta:0', 'bert/encoder/layer_._10/attention/output/dense/kernel:0', 'bert/encoder/layer_._3/output/LayerNorm/gamma:0', 'bert/encoder/layer_._12/attention/output/dense/bias:0']\n",
      "- This IS expected if you are initializing TFBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing TFBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some layers of TFBertModel were not initialized from the model checkpoint at bert-large-uncased and are newly initialized: ['']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, TFBertModel\n",
    "tokenizer = AutoTokenizer.from_pretrained('bert-large-uncased')\n",
    "bert = TFBertModel.from_pretrained('bert-large-uncased')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max len of tweet 31\n"
     ]
    }
   ],
   "source": [
    "print(\"max len of tweet\", max([len(x.split()) for x in train_df.text]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "max_len = 34"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = tokenizer(\n",
    "    text = train_df.text.tolist(),\n",
    "    add_special_tokens=True,\n",
    "    max_length=max_len,\n",
    "    truncation= True,\n",
    "    padding = True,\n",
    "    return_tensors = 'tf',\n",
    "    return_token_type_ids= False,\n",
    "    return_attention_mask=True,\n",
    "    verbose= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': <tf.Tensor: shape=(7613, 34), dtype=int32, numpy=\n",
       "array([[  101,  2256, 15616, ...,     0,     0,     0],\n",
       "       [  101,  3224,  2543, ...,     0,     0,     0],\n",
       "       [  101,  2035,  3901, ...,     0,     0,     0],\n",
       "       ...,\n",
       "       [  101, 23290,  2683, ...,     0,     0,     0],\n",
       "       [  101,  2610, 11538, ...,     0,     0,     0],\n",
       "       [  101,  1996,  6745, ...,     0,     0,     0]])>, 'attention_mask': <tf.Tensor: shape=(7613, 34), dtype=int32, numpy=\n",
       "array([[1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0],\n",
       "       [1, 1, 1, ..., 0, 0, 0]])>}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['input_ids'].shape\n",
    "x_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TensorShape([7613, 34])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train['attention_mask'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, ..., 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train = train_df.target.values\n",
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming x_train is your tokenized input from earlier\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.optimizers import Adam\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.initializers import TruncatedNormal\n",
    "from keras.losses import CategoricalCrossentropy,BinaryCrossentropy\n",
    "from keras.metrics import CategoricalAccuracy,BinaryAccuracy\n",
    "from keras.utils import to_categorical\n",
    "from keras.utils import plot_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    4342\n",
       "1    3271\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df.target.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "train_dataset = tf.data.Dataset.from_tensor_slices((\n",
    "    {\n",
    "        \"input_ids\": x_train[\"input_ids\"],\n",
    "        \"attention_mask\": x_train[\"attention_mask\"]\n",
    "    },\n",
    "    train_df.target.values\n",
    "))\n",
    "\n",
    "# Batch and prefetch for performance\n",
    "train_dataset = train_dataset.batch(32).prefetch(tf.data.AUTOTUNE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from transformers import TFBertModel\n",
    "\n",
    "# Load pre-trained BERT model\n",
    "bert = TFBertModel.from_pretrained('bert-large-uncased')\n",
    "\n",
    "# Define the model as a tf.keras.Model subclass\n",
    "class BertClassifier(tf.keras.Model):\n",
    "    def __init__(self, bert_model):\n",
    "        super(BertClassifier, self).__init__()\n",
    "        self.bert = bert_model\n",
    "        self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "        self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "        self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
    "        self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "    def call(self, inputs, training=False):\n",
    "        input_ids, attention_mask = inputs\n",
    "        bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "        pooled_output = bert_output.pooler_output\n",
    "        x = self.dropout1(pooled_output, training=training)\n",
    "        x = self.dense1(x)\n",
    "        x = self.dropout2(x, training=training)\n",
    "        x = self.dense2(x)\n",
    "        return self.output_layer(x)\n",
    "\n",
    "# Instantiate the model\n",
    "model = BertClassifier(bert)\n",
    "\n",
    "# Define loss function and optimizer\n",
    "loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "# Custom accuracy function\n",
    "def binary_accuracy(y_true, y_pred):\n",
    "    y_pred = tf.round(y_pred)\n",
    "    return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "\n",
    "# Define training step\n",
    "@tf.function\n",
    "def train_step(input_ids, attention_mask, labels):\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions = model((input_ids, attention_mask), training=True)\n",
    "        loss = loss_object(labels, predictions)\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    accuracy = binary_accuracy(labels, predictions)\n",
    "    return loss, accuracy\n",
    "\n",
    "# Prepare your data\n",
    "# Assuming x_train is your tokenized input from earlier\n",
    "input_ids = tf.convert_to_tensor(x_train[\"input_ids\"], dtype=tf.int32)\n",
    "attention_mask = tf.convert_to_tensor(x_train[\"attention_mask\"], dtype=tf.int32)\n",
    "labels = tf.convert_to_tensor(train_df.target.values, dtype=tf.float32)\n",
    "\n",
    "# Create tf.data.Dataset\n",
    "dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_mask, labels))\n",
    "dataset = dataset.shuffle(buffer_size=len(input_ids)).batch(32)\n",
    "\n",
    "# Training loop\n",
    "epochs = 10\n",
    "for epoch in range(epochs):\n",
    "    total_loss = 0\n",
    "    total_accuracy = 0\n",
    "    num_batches = 0\n",
    "    \n",
    "    for batch in dataset:\n",
    "        input_ids_batch, attention_mask_batch, labels_batch = batch\n",
    "        loss, accuracy = train_step(input_ids_batch, attention_mask_batch, labels_batch)\n",
    "        total_loss += loss\n",
    "        total_accuracy += accuracy\n",
    "        num_batches += 1\n",
    "    \n",
    "    avg_loss = total_loss / num_batches\n",
    "    avg_accuracy = total_accuracy / num_batches\n",
    "    \n",
    "    print(f'Epoch {epoch + 1}, '\n",
    "          f'Loss: {avg_loss:.4f}, '\n",
    "          f'Accuracy: {avg_accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\autograph\\converters\\directives.py:126: The name tf.nn.fused_batch_norm is deprecated. Please use tf.compat.v1.nn.fused_batch_norm instead.\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epochs:   0%|          | 0/10 [01:36<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[46], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m batch_progress_bar:\n\u001b[0;32m     79\u001b[0m     input_ids_batch, attention_mask_batch, labels_batch \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m---> 80\u001b[0m     loss, accuracy \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattention_mask_batch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels_batch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     total_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\n\u001b[0;32m     82\u001b[0m     total_accuracy \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m accuracy\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\util\\traceback_utils.py:150\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    148\u001b[0m filtered_tb \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    149\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 150\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:833\u001b[0m, in \u001b[0;36mFunction.__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    830\u001b[0m compiler \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mxla\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnonXla\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    832\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m OptionalXlaContext(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_jit_compile):\n\u001b[1;32m--> 833\u001b[0m   result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    835\u001b[0m new_tracing_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mexperimental_get_tracing_count()\n\u001b[0;32m    836\u001b[0m without_tracing \u001b[38;5;241m=\u001b[39m (tracing_count \u001b[38;5;241m==\u001b[39m new_tracing_count)\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\polymorphic_function.py:869\u001b[0m, in \u001b[0;36mFunction._call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    866\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[0;32m    867\u001b[0m   \u001b[38;5;66;03m# In this case we have created variables on the first call, so we run the\u001b[39;00m\n\u001b[0;32m    868\u001b[0m   \u001b[38;5;66;03m# defunned version which is guaranteed to never create variables.\u001b[39;00m\n\u001b[1;32m--> 869\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtracing_compilation\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    870\u001b[0m \u001b[43m      \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_no_variable_creation_config\u001b[49m\n\u001b[0;32m    871\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    872\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_variable_creation_config \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    873\u001b[0m   \u001b[38;5;66;03m# Release the lock early so that multiple threads can perform the call\u001b[39;00m\n\u001b[0;32m    874\u001b[0m   \u001b[38;5;66;03m# in parallel.\u001b[39;00m\n\u001b[0;32m    875\u001b[0m   \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_lock\u001b[38;5;241m.\u001b[39mrelease()\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\tracing_compilation.py:139\u001b[0m, in \u001b[0;36mcall_function\u001b[1;34m(args, kwargs, tracing_options)\u001b[0m\n\u001b[0;32m    137\u001b[0m bound_args \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mbind(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    138\u001b[0m flat_inputs \u001b[38;5;241m=\u001b[39m function\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39munpack_inputs(bound_args)\n\u001b[1;32m--> 139\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# pylint: disable=protected-access\u001b[39;49;00m\n\u001b[0;32m    140\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcaptured_inputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfunction\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcaptured_inputs\u001b[49m\n\u001b[0;32m    141\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\concrete_function.py:1322\u001b[0m, in \u001b[0;36mConcreteFunction._call_flat\u001b[1;34m(self, tensor_inputs, captured_inputs)\u001b[0m\n\u001b[0;32m   1318\u001b[0m possible_gradient_type \u001b[38;5;241m=\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPossibleTapeGradientTypes(args)\n\u001b[0;32m   1319\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (possible_gradient_type \u001b[38;5;241m==\u001b[39m gradients_util\u001b[38;5;241m.\u001b[39mPOSSIBLE_GRADIENT_TYPES_NONE\n\u001b[0;32m   1320\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m executing_eagerly):\n\u001b[0;32m   1321\u001b[0m   \u001b[38;5;66;03m# No tape is watching; skip to running the function.\u001b[39;00m\n\u001b[1;32m-> 1322\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_inference_function\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_preflattened\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1323\u001b[0m forward_backward \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_select_forward_and_backward_functions(\n\u001b[0;32m   1324\u001b[0m     args,\n\u001b[0;32m   1325\u001b[0m     possible_gradient_type,\n\u001b[0;32m   1326\u001b[0m     executing_eagerly)\n\u001b[0;32m   1327\u001b[0m forward_function, args_with_tangents \u001b[38;5;241m=\u001b[39m forward_backward\u001b[38;5;241m.\u001b[39mforward()\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:216\u001b[0m, in \u001b[0;36mAtomicFunction.call_preflattened\u001b[1;34m(self, args)\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcall_preflattened\u001b[39m(\u001b[38;5;28mself\u001b[39m, args: Sequence[core\u001b[38;5;241m.\u001b[39mTensor]) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Any:\n\u001b[0;32m    215\u001b[0m \u001b[38;5;250m  \u001b[39m\u001b[38;5;124;03m\"\"\"Calls with flattened tensor inputs and returns the structured output.\"\"\"\u001b[39;00m\n\u001b[1;32m--> 216\u001b[0m   flat_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_flat\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    217\u001b[0m   \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfunction_type\u001b[38;5;241m.\u001b[39mpack_output(flat_outputs)\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\polymorphic_function\\atomic_function.py:251\u001b[0m, in \u001b[0;36mAtomicFunction.call_flat\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    249\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m record\u001b[38;5;241m.\u001b[39mstop_recording():\n\u001b[0;32m    250\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mexecuting_eagerly():\n\u001b[1;32m--> 251\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_bound_context\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcall_function\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    252\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    253\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    254\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfunction_type\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflat_outputs\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    255\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    256\u001b[0m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    257\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m make_call_op_in_graph(\n\u001b[0;32m    258\u001b[0m         \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m    259\u001b[0m         \u001b[38;5;28mlist\u001b[39m(args),\n\u001b[0;32m    260\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_bound_context\u001b[38;5;241m.\u001b[39mfunction_call_options\u001b[38;5;241m.\u001b[39mas_attrs(),\n\u001b[0;32m    261\u001b[0m     )\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\context.py:1500\u001b[0m, in \u001b[0;36mContext.call_function\u001b[1;34m(self, name, tensor_inputs, num_outputs)\u001b[0m\n\u001b[0;32m   1498\u001b[0m cancellation_context \u001b[38;5;241m=\u001b[39m cancellation\u001b[38;5;241m.\u001b[39mcontext()\n\u001b[0;32m   1499\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m cancellation_context \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1500\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m \u001b[43mexecute\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1501\u001b[0m \u001b[43m      \u001b[49m\u001b[43mname\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecode\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mutf-8\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1502\u001b[0m \u001b[43m      \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_outputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1503\u001b[0m \u001b[43m      \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtensor_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1504\u001b[0m \u001b[43m      \u001b[49m\u001b[43mattrs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1505\u001b[0m \u001b[43m      \u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1506\u001b[0m \u001b[43m  \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1507\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1508\u001b[0m   outputs \u001b[38;5;241m=\u001b[39m execute\u001b[38;5;241m.\u001b[39mexecute_with_cancellation(\n\u001b[0;32m   1509\u001b[0m       name\u001b[38;5;241m.\u001b[39mdecode(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mutf-8\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1510\u001b[0m       num_outputs\u001b[38;5;241m=\u001b[39mnum_outputs,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1514\u001b[0m       cancellation_manager\u001b[38;5;241m=\u001b[39mcancellation_context,\n\u001b[0;32m   1515\u001b[0m   )\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tensorflow\\python\\eager\\execute.py:53\u001b[0m, in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     51\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m     52\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[1;32m---> 53\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m \u001b[43mpywrap_tfe\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mTFE_Py_Execute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mctx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_handle\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mop_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     54\u001b[0m \u001b[43m                                      \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mattrs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     55\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[0;32m     56\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# import tensorflow as tf\n",
    "# from transformers import TFBertModel\n",
    "# from tqdm import tqdm\n",
    "# # Load pre-trained BERT model\n",
    "\n",
    "\n",
    "# # Define the model as a tf.keras.Model subclass\n",
    "# class BertClassifier(tf.keras.Model):\n",
    "#     def __init__(self, bert_model):\n",
    "#         super(BertClassifier, self).__init__()\n",
    "#         self.bert = bert_model\n",
    "#         self.dropout1 = tf.keras.layers.Dropout(0.1)\n",
    "#         self.dense1 = tf.keras.layers.Dense(128, activation='relu')\n",
    "#         self.dropout2 = tf.keras.layers.Dropout(0.1)\n",
    "#         self.dense2 = tf.keras.layers.Dense(32, activation='relu')\n",
    "#         self.output_layer = tf.keras.layers.Dense(1, activation='sigmoid')\n",
    "\n",
    "#     def call(self, inputs, training=False):\n",
    "#         input_ids, attention_mask = inputs\n",
    "#         bert_output = self.bert(input_ids=input_ids, attention_mask=attention_mask, training=training)\n",
    "#         pooled_output = bert_output.pooler_output\n",
    "#         x = self.dropout1(pooled_output, training=training)\n",
    "#         x = self.dense1(x)\n",
    "#         x = self.dropout2(x, training=training)\n",
    "#         x = self.dense2(x)\n",
    "#         return self.output_layer(x)\n",
    "\n",
    "# # Instantiate the model\n",
    "# model = BertClassifier(bert)\n",
    "\n",
    "# # Define loss function and optimizer\n",
    "# loss_object = tf.keras.losses.BinaryCrossentropy()\n",
    "# optimizer = tf.keras.optimizers.Adam(learning_rate=1e-5)\n",
    "\n",
    "# # Custom accuracy function\n",
    "# def binary_accuracy(y_true, y_pred):\n",
    "#     y_pred = tf.round(y_pred)\n",
    "#     return tf.reduce_mean(tf.cast(tf.equal(y_true, y_pred), tf.float32))\n",
    "\n",
    "# # Define training step\n",
    "# @tf.function\n",
    "# def train_step(input_ids, attention_mask, labels):\n",
    "#     with tf.GradientTape() as tape:\n",
    "#         predictions = model((input_ids, attention_mask), training=True)\n",
    "#         loss = loss_object(labels, predictions)\n",
    "#     gradients = tape.gradient(loss, model.trainable_variables)\n",
    "#     optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "#     accuracy = binary_accuracy(labels, predictions)\n",
    "#     return loss, accuracy\n",
    "\n",
    "# # Prepare your data\n",
    "# # Assuming x_train is your tokenized input from earlier\n",
    "# input_ids = tf.convert_to_tensor(x_train[\"input_ids\"], dtype=tf.int32)\n",
    "# attention_mask = tf.convert_to_tensor(x_train[\"attention_mask\"], dtype=tf.int32)\n",
    "# labels = tf.convert_to_tensor(train_df.target.values, dtype=tf.float32)\n",
    "\n",
    "# # Create tf.data.Dataset\n",
    "# dataset = tf.data.Dataset.from_tensor_slices((input_ids, attention_mask, labels))\n",
    "# dataset = dataset.shuffle(buffer_size=len(input_ids)).batch(32)\n",
    "\n",
    "# # Training loop\n",
    "# # Training loop\n",
    "# epochs = 10\n",
    "# best_accuracy = 0\n",
    "# save_path = 'saved_model'\n",
    "\n",
    "# # Create a progress bar for epochs\n",
    "# epoch_progress_bar = tqdm(range(epochs), desc=\"Epochs\", position=0)\n",
    "\n",
    "# for epoch in epoch_progress_bar:\n",
    "#     total_loss = 0\n",
    "#     total_accuracy = 0\n",
    "#     num_batches = 0\n",
    "    \n",
    "#     # Create a progress bar for batches\n",
    "#     batch_progress_bar = tqdm(dataset, desc=f\"Epoch {epoch+1}\", leave=False, position=1)\n",
    "    \n",
    "#     for batch in batch_progress_bar:\n",
    "#         input_ids_batch, attention_mask_batch, labels_batch = batch\n",
    "#         loss, accuracy = train_step(input_ids_batch, attention_mask_batch, labels_batch)\n",
    "#         total_loss += loss\n",
    "#         total_accuracy += accuracy\n",
    "#         num_batches += 1\n",
    "        \n",
    "#         # Update batch progress bar\n",
    "#         batch_progress_bar.set_postfix({\"Loss\": f\"{loss:.4f}\", \"Accuracy\": f\"{accuracy:.4f}\"})\n",
    "    \n",
    "#     avg_loss = total_loss / num_batches\n",
    "#     avg_accuracy = total_accuracy / num_batches\n",
    "    \n",
    "#     # Update epoch progress bar\n",
    "#     epoch_progress_bar.set_postfix({\"Avg Loss\": f\"{avg_loss:.4f}\", \"Avg Accuracy\": f\"{avg_accuracy:.4f}\"})\n",
    "    \n",
    "#     # Save the model if it's the best so far\n",
    "#     if avg_accuracy > best_accuracy:\n",
    "#         best_accuracy = avg_accuracy\n",
    "        \n",
    "#         # Save the entire model\n",
    "#         tf.keras.models.save_model(model, save_path)\n",
    "#         print(f\"\\nModel saved to {save_path}\")\n",
    "\n",
    "# print(\"\\nTraining completed. Best model saved.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend\\tensorflow\\core.py:184: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Exception encountered when calling TFBertModel.call().\n\n\u001b[1mAll operation outputs must be tensors. Operation <TFBertModel name=tf_bert_model, built=True> returned a non-tensor. Non-tensor received: TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor 'bert_1/encoder_1/layer_._23_1/output_1/LayerNorm_1/add_2:0' shape=(None, 34, 1024) dtype=float32>, pooler_output=<tf.Tensor 'bert_1/pooler_1/dense_1/Tanh:0' shape=(None, 1024) dtype=float32>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\u001b[0m\n\nArguments received by TFBertModel.call():\n  • args=('<KerasTensor shape=(None, 34), dtype=int32, sparse=None, name=input_ids>',)\n  • kwargs={'attention_mask': '<KerasTensor shape=(None, 34), dtype=int32, sparse=None, name=attention_mask>', 'training': 'False'}",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[18], line 9\u001b[0m\n\u001b[0;32m      5\u001b[0m input_mask \u001b[38;5;241m=\u001b[39m Input(shape\u001b[38;5;241m=\u001b[39m(max_len,), dtype\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mint32, name\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m embeddings \u001b[38;5;241m=\u001b[39m \u001b[43mbert\u001b[49m\u001b[43m(\u001b[49m\u001b[43minput_ids\u001b[49m\u001b[43m,\u001b[49m\u001b[43mattention_mask\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43minput_mask\u001b[49m\u001b[43m)\u001b[49m[\u001b[38;5;241m1\u001b[39m] \u001b[38;5;66;03m#(0 is the last hidden states,1 means pooler_output)\u001b[39;00m\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\u001b[39;00m\n\u001b[0;32m     11\u001b[0m out \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39mlayers\u001b[38;5;241m.\u001b[39mDropout(\u001b[38;5;241m0.1\u001b[39m)(embeddings)\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\Soham R\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\ops\\node.py:48\u001b[0m, in \u001b[0;36mNode.__init__\u001b[1;34m(self, operation, call_args, call_kwargs, outputs)\u001b[0m\n\u001b[0;32m     46\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moutputs:\n\u001b[0;32m     47\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(x, KerasTensor):\n\u001b[1;32m---> 48\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m     49\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAll operation outputs must be tensors. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     50\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOperation \u001b[39m\u001b[38;5;132;01m{\u001b[39;00moperation\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m returned a non-tensor. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     51\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNon-tensor received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mx\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     52\u001b[0m         )\n\u001b[0;32m     54\u001b[0m zero_history \u001b[38;5;241m=\u001b[39m \u001b[38;5;28many\u001b[39m(\n\u001b[0;32m     55\u001b[0m     \u001b[38;5;129;01mnot\u001b[39;00m x\u001b[38;5;241m.\u001b[39mrecord_history \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39marguments\u001b[38;5;241m.\u001b[39mkeras_tensors\n\u001b[0;32m     56\u001b[0m )\n\u001b[0;32m     58\u001b[0m \u001b[38;5;66;03m# If inputs don't have metadata yet, add it.\u001b[39;00m\n",
      "\u001b[1;31mValueError\u001b[0m: Exception encountered when calling TFBertModel.call().\n\n\u001b[1mAll operation outputs must be tensors. Operation <TFBertModel name=tf_bert_model, built=True> returned a non-tensor. Non-tensor received: TFBaseModelOutputWithPoolingAndCrossAttentions(last_hidden_state=<tf.Tensor 'bert_1/encoder_1/layer_._23_1/output_1/LayerNorm_1/add_2:0' shape=(None, 34, 1024) dtype=float32>, pooler_output=<tf.Tensor 'bert_1/pooler_1/dense_1/Tanh:0' shape=(None, 1024) dtype=float32>, past_key_values=None, hidden_states=None, attentions=None, cross_attentions=None)\u001b[0m\n\nArguments received by TFBertModel.call():\n  • args=('<KerasTensor shape=(None, 34), dtype=int32, sparse=None, name=input_ids>',)\n  • kwargs={'attention_mask': '<KerasTensor shape=(None, 34), dtype=int32, sparse=None, name=attention_mask>', 'training': 'False'}"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "\n",
    "input_ids = Input(shape=(max_len,), dtype=tf.int32, name=\"input_ids\")\n",
    "input_mask = Input(shape=(max_len,), dtype=tf.int32, name=\"attention_mask\")\n",
    "# embeddings = dbert_model(input_ids,attention_mask = input_mask)[0]\n",
    "\n",
    "\n",
    "embeddings = bert(input_ids,attention_mask = input_mask)[1] #(0 is the last hidden states,1 means pooler_output)\n",
    "# out = tf.keras.layers.GlobalMaxPool1D()(embeddings)\n",
    "out = tf.keras.layers.Dropout(0.1)(embeddings)\n",
    "\n",
    "out = Dense(128, activation='relu')(out)\n",
    "out = tf.keras.layers.Dropout(0.1)(out)\n",
    "out = Dense(32,activation = 'relu')(out)\n",
    "\n",
    "y = Dense(1,activation = 'sigmoid')(out)\n",
    "    \n",
    "model = tf.keras.Model(inputs=[input_ids, input_mask], outputs=y)\n",
    "model.layers[2].trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
