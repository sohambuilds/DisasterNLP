{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForTokenClassification, pipeline\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tag import pos_tag\n",
    "import random\n",
    "\n",
    "\n",
    "\n",
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "\n",
    "\n",
    "\n",
    "ner_model_name = \"dbmdz/bert-large-cased-finetuned-conll03-english\"\n",
    "ner = pipeline(\"ner\", model=ner_model_name, aggregation_strategy=\"simple\")\n",
    "\n",
    "\n",
    "disaster_words = set([\"earthquake\", \"flood\", \"cyclone\", \"landslide\", \"drought\", \"heatwave\", \"thunderstorm\", \"cloudburst\", \"tornado\", \"tsunami\"])\n",
    "\n",
    "\n",
    "indian_cities = [\n",
    "    \"Mumbai\", \"Delhi\", \"Bangalore\", \"Hyderabad\", \"Chennai\", \"Kolkata\", \"Pune\", \"Ahmedabad\", \"Jaipur\", \"Lucknow\",\n",
    "    \"Kanpur\", \"Nagpur\", \"Indore\", \"Thane\", \"Bhopal\", \"Visakhapatnam\", \"Pimpri-Chinchwad\", \"Patna\", \"Vadodara\", \"Ghaziabad\",\n",
    "    \"Ludhiana\", \"Agra\", \"Nashik\", \"Faridabad\", \"Meerut\", \"Rajkot\", \"Kalyan-Dombivli\", \"Vasai-Virar\", \"Varanasi\", \"Srinagar\",\n",
    "    \"Aurangabad\", \"Dhanbad\", \"Amritsar\", \"Navi Mumbai\", \"Allahabad\", \"Ranchi\", \"Howrah\", \"Coimbatore\", \"Jabalpur\", \"Gwalior\",\n",
    "    \"Vijayawada\", \"Jodhpur\", \"Madurai\", \"Raipur\", \"Kota\", \"Guwahati\", \"Chandigarh\", \"Solapur\", \"Hubballi-Dharwad\", \"Tiruchirappalli\"\n",
    "]\n",
    "\n",
    "def generate_indian_disaster_tweets(num_tweets=1000):\n",
    "    disaster_templates = [\n",
    "        \"Massive {} hits {}! People struggling to cope.\",\n",
    "        \"Breaking: {} strikes {} causing widespread panic.\",\n",
    "        \"{} in {} leaves thousands stranded. Govt. agencies on high alert.\",\n",
    "        \"Unexpected {} catches {} off guard. Emergency services overwhelmed.\",\n",
    "        \"{} warning issued for {}. Residents advised to stay indoors.\",\n",
    "        \"{}. {} reels under nature's fury. CM announces relief measures.\",\n",
    "        \"{}. Chaos in {} as authorities scramble to respond.\",\n",
    "        \"{} wreaks havoc in {}. Schools and offices closed.\",\n",
    "        \"{}. {} faces worst disaster in decades. PM assures all help.\",\n",
    "        \"Terrifying {} hits {}. Social media flooded with rescue requests.\"\n",
    "    ]\n",
    "\n",
    "    non_disaster_templates = [\n",
    "        \"{} gears up for festive season. Markets buzzing with activity.\",\n",
    "        \"New metro line inaugurated in {}. Commuters rejoice!\",\n",
    "        \"{} hosts tech summit. Startups showcase cutting-edge innovations.\",\n",
    "        \"Annual food festival kicks off in {}. Foodies flock in large numbers.\",\n",
    "        \"{} sets new record in cleanliness drive. Mayor lauds citizen effort.\",\n",
    "        \"Cultural extravaganza in {} draws tourists from across the globe.\",\n",
    "        \"{}. Students shine in board exams. Parents and teachers proud.\",\n",
    "        \"Traffic woes in {} as new flyover construction begins.\",\n",
    "        \"{}. Local team clinches victory in inter-city cricket tournament.\",\n",
    "        \"Art exhibition in {} showcases rich cultural heritage.\"\n",
    "    ]\n",
    "\n",
    "    tweets = []\n",
    "    for _ in range(num_tweets):\n",
    "        if random.random() < 0.7:  #\n",
    "            template = random.choice(disaster_templates)\n",
    "            disaster = random.choice(list(disaster_words)).capitalize()\n",
    "            city = random.choice(indian_cities)\n",
    "            tweet = template.format(disaster, city)\n",
    "        else:\n",
    "            template = random.choice(non_disaster_templates)\n",
    "            city = random.choice(indian_cities)\n",
    "            tweet = template.format(city)\n",
    "        tweets.append(tweet)\n",
    "    \n",
    "    return tweets\n",
    "\n",
    "corpus = generate_indian_disaster_tweets(5000)\n",
    "\n",
    "\n",
    "def extract_location(tweet):\n",
    "    entities = ner(tweet)\n",
    "    locations = [entity['word'] for entity in entities if entity['entity_group'] in ['LOC', 'GPE']]\n",
    "    return locations[0] if locations else None\n",
    "\n",
    "def extract_disaster_type(tweet, corpus):\n",
    "    # Tokenize and POS tag\n",
    "    tokens = word_tokenize(tweet.lower())\n",
    "    pos_tags = pos_tag(tokens)\n",
    "    \n",
    "    # Keep only nouns and verbs\n",
    "    filtered_tokens = [word for word, pos in pos_tags if pos.startswith('NN') or pos.startswith('VB')]\n",
    "    \n",
    "    # Calculate TF-IDF\n",
    "    vectorizer = TfidfVectorizer()\n",
    "    tfidf_matrix = vectorizer.fit_transform(corpus + [tweet])\n",
    "    feature_names = vectorizer.get_feature_names_out()\n",
    "    \n",
    "    # Get TF-IDF scores for the last document (our tweet)\n",
    "    tfidf_scores = dict(zip(feature_names, tfidf_matrix.toarray()[-1]))\n",
    "    \n",
    "    # Custom scoring function\n",
    "    def score_word(word):\n",
    "        base_score = tfidf_scores.get(word, 0)\n",
    "        if word in disaster_words:\n",
    "            base_score *= 2  # Double the score for known disaster words\n",
    "        return base_score\n",
    "    \n",
    "    # Score words based on custom scoring function\n",
    "    scored_words = [(word, score_word(word)) for word in filtered_tokens]\n",
    "    \n",
    "    # Sort by score and get top word\n",
    "    top_words = sorted(scored_words, key=lambda x: x[1], reverse=True)\n",
    "    \n",
    "    # Return the highest-scoring word that's in our disaster_words set, if any\n",
    "    for word, score in top_words:\n",
    "        if word in disaster_words:\n",
    "            return word\n",
    "    \n",
    "    # If no disaster word is found, return the top-scoring word\n",
    "    return top_words[0][0] if top_words else None\n",
    "\n",
    "# Test tweets\n",
    "test_tweets = [\n",
    "    \"Massive earthquake hits Mumbai! People struggling to cope.\",\n",
    "    \"Breaking: Cyclone strikes Chennai causing widespread panic.\",\n",
    "    \"Flood in Patna leaves thousands stranded. Govt. agencies on high alert.\",\n",
    "    \"Unexpected landslide catches Shimla off guard. Emergency services overwhelmed.\",\n",
    "    \"Drought warning issued for Marathwada. Residents advised to conserve water.\",\n",
    "    \"Bangalore gears up for festive season. Markets buzzing with activity.\",\n",
    "    \"New metro line inaugurated in Delhi. Commuters rejoice!\",\n",
    "    \"Ahmedabad hosts tech summit. Startups showcase cutting-edge innovations.\",\n",
    "    \"Heatwave. Nagpur reels under nature's fury. CM announces relief measures.\",\n",
    "    \"Cloudburst wreaks havoc in Uttarakhand. Schools and offices closed.\"\n",
    "]\n",
    "\n",
    "for tweet in test_tweets:\n",
    "    disaster_type = extract_disaster_type(tweet, corpus)\n",
    "    location = extract_location(tweet)\n",
    "    print(f\"Tweet: {tweet}\")\n",
    "    print(f\"Disaster Type: {disaster_type}\")\n",
    "    print(f\"Location: {location}\\n\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
